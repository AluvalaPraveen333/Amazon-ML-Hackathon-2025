{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5T7lVQzwZECogZV6ekQBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munnurumahesh03-coder/Amazon-ML-Hackathon-2025/blob/main/Final_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importings**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_IlRuy_iN5a8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TL-wESVPNcIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233f747a-29db-423b-ba22-aa427808a092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully.\n",
            "Data loaded successfully. Train shape: (75000, 4), Test shape: (75000, 3)\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: SETUP, DATA LOADING, AND CONSTANTS\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"All libraries imported successfully.\")\n",
        "\n",
        "# --- Constants ---\n",
        "TRAIN_FILE_PATH = \"train.csv\"\n",
        "TEST_FILE_PATH = \"test.csv\"\n",
        "SAMPLE_ID_COL = 'sample_id'\n",
        "TEXT_COL = 'catalog_content'\n",
        "TARGET_COL = 'price'\n",
        "BRAND_COL = 'brand'\n",
        "RANDOM_STATE = 42\n",
        "TFIDF_MAX_FEATURES = 8000 # Increased for more text detail\n",
        "N_SPLITS = 5 # Using 5-fold CV for robust validation\n",
        "\n",
        "# --- Load Data ---\n",
        "try:\n",
        "    train_df = pd.read_csv(TRAIN_FILE_PATH)\n",
        "    test_df = pd.read_csv(TEST_FILE_PATH)\n",
        "    print(f\"Data loaded successfully. Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Data files not found. Please check file paths.\")\n",
        "\n",
        "# --- Define SMAPE Metric ---\n",
        "def smape(y_true, y_pred):\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE ULTIMATE FEATURE FACTORY**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Nf6bK4veN_A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: THE ULTIMATE FEATURE FACTORY\n",
        "# =============================================================================\n",
        "print(\"--- Starting the Ultimate Feature Factory ---\")\n",
        "\n",
        "def create_all_features(df, train_df_for_stats=None):\n",
        "    \"\"\"\n",
        "    This single, powerful function creates all engineered features.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # --- 1. Core Feature Extraction (from original notebook) ---\n",
        "    item_name = df_copy[TEXT_COL].str.extract(r'Item Name:\\s*(.*)', flags=re.IGNORECASE).iloc[:, 0].fillna('')\n",
        "\n",
        "    # Brand\n",
        "    NON_BRAND_WORDS = ['The', 'A', 'An', 'Organic', 'Gluten-Free', 'Natural', 'Pure', 'Food', 'Gourmet', 'Simply', 'And', 'For', 'With', 'Pack', 'To', 'Of', 'In', 'From', 'By', 'On', 'At', 'Is', 'It']\n",
        "    def get_brand(name):\n",
        "        if not isinstance(name, str) or not name: return 'unknown'\n",
        "        words = name.split(); first_word = words[0].title().replace(',', '')\n",
        "        if first_word in NON_BRAND_WORDS and len(words) > 1: return words[1].title().replace(',', '')\n",
        "        return first_word\n",
        "    df_copy[BRAND_COL] = item_name.apply(get_brand)\n",
        "\n",
        "    # Weight, Volume, Pack Count (simplified and robust)\n",
        "    df_copy['weight_grams'] = item_name.str.extract(r'(\\d+\\.?\\d*)\\s*(?:g|gram|grams)\\b', flags=re.IGNORECASE)[0].astype(float)\n",
        "    df_copy['pack_count'] = item_name.str.extract(r'(?:pack of|count of|pk of|\\s)(\\d+)\\s*(?:ct|count|pack)', flags=re.IGNORECASE)[0].astype(float)\n",
        "\n",
        "    # --- 2. Advanced Text Statistic Features ---\n",
        "    df_copy['name_char_count'] = item_name.str.len()\n",
        "    df_copy['name_word_count'] = item_name.str.split().str.len()\n",
        "    df_copy['name_all_caps_word_count'] = item_name.str.findall(r'\\b[A-Z]{2,}\\b').str.len()\n",
        "\n",
        "    # --- 3. Advanced Keyword Flag Features ---\n",
        "    df_copy['is_organic'] = item_name.str.contains('organic', case=False).astype(int)\n",
        "    df_copy['is_gluten_free'] = item_name.str.contains('gluten free|gluten-free', case=False).astype(int)\n",
        "    df_copy['is_case'] = item_name.str.contains(r'\\bcase\\b', case=False).astype(int)\n",
        "    df_copy['is_kosher'] = item_name.str.contains('kosher', case=False).astype(int)\n",
        "\n",
        "    # --- 4. Brand-Based Statistical Features ---\n",
        "    if train_df_for_stats is not None:\n",
        "        brand_stats = train_df_for_stats.groupby(BRAND_COL).agg(\n",
        "            brand_avg_price=('price', 'mean'),\n",
        "            brand_product_count=(SAMPLE_ID_COL, 'count')\n",
        "        ).reset_index()\n",
        "        df_copy = pd.merge(df_copy, brand_stats, on=BRAND_COL, how='left')\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# --- Apply the Feature Factory ---\n",
        "# Create stats from the original training data\n",
        "train_for_stats = create_all_features(train_df.copy())\n",
        "\n",
        "# Apply to create the final, feature-rich dataframes\n",
        "train_featured = create_all_features(train_df.copy(), train_for_stats)\n",
        "test_featured = create_all_features(test_df.copy(), train_for_stats)\n",
        "\n",
        "print(\"Ultimate Feature Factory complete.\")\n",
        "print(\"New training data shape:\", train_featured.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au2GM_piNxyc",
        "outputId": "6b8198e8-bb56-4c2f-daf9-be1d7883fc23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting the Ultimate Feature Factory ---\n",
            "Ultimate Feature Factory complete.\n",
            "New training data shape: (75000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA CLEANING AND PREPROCESSING PIPELINE**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tXvZzuj-OM75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: DATA CLEANING AND PREPROCESSING\n",
        "# =============================================================================\n",
        "print(\"--- Starting Data Cleaning and Preprocessing ---\")\n",
        "\n",
        "# --- 1. Data Cleaning ---\n",
        "# Remove placeholder images identified in previous notebooks\n",
        "image_counts = train_featured['image_link'].value_counts()\n",
        "placeholders = image_counts[image_counts > 10].index\n",
        "train_cleaned = train_featured[~train_featured['image_link'].isin(placeholders)]\n",
        "\n",
        "# Clean target variable (price)\n",
        "train_cleaned[TARGET_COL] = pd.to_numeric(train_cleaned[TARGET_COL], errors='coerce')\n",
        "train_cleaned.dropna(subset=[TARGET_COL], inplace=True)\n",
        "print(f\"Data cleaned. Final training rows: {len(train_cleaned)}\")\n",
        "\n",
        "# --- 2. Define Feature Lists for the Pipeline ---\n",
        "numeric_features = [\n",
        "    'weight_grams', 'pack_count', 'name_char_count', 'name_word_count',\n",
        "    'name_all_caps_word_count', 'is_organic', 'is_gluten_free', 'is_case', 'is_kosher',\n",
        "    'brand_avg_price', 'brand_product_count'\n",
        "]\n",
        "categorical_features = [BRAND_COL]\n",
        "text_feature = TEXT_COL\n",
        "\n",
        "# --- 3. Build the Preprocessing Pipeline ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))]), numeric_features),\n",
        "        ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_features),\n",
        "        ('text', TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words='english', ngram_range=(1, 2)), text_feature)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "transform_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', StandardScaler(with_mean=False)) # Scale all features\n",
        "])\n",
        "\n",
        "# --- 4. Prepare Data for Modeling ---\n",
        "X = train_cleaned[numeric_features + categorical_features + [text_feature]]\n",
        "y = np.log1p(train_cleaned[TARGET_COL])\n",
        "\n",
        "X_test = test_featured[numeric_features + categorical_features + [text_feature]]\n",
        "\n",
        "# Fit the pipeline on training data and transform both train and test\n",
        "X_sparse = transform_pipeline.fit_transform(X)\n",
        "y_numpy = y.values\n",
        "X_test_sparse = transform_pipeline.transform(X_test)\n",
        "\n",
        "print(\"Preprocessing complete. Data is ready for modeling.\")\n",
        "print(f\"Final feature matrix shape: {X_sparse.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWP_7QRPNxu-",
        "outputId": "946fb4eb-1d23-4d78-bcc1-f44256301fcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Cleaning and Preprocessing ---\n",
            "Data cleaned. Final training rows: 74891\n",
            "Preprocessing complete. Data is ready for modeling.\n",
            "Final feature matrix shape: (74891, 17349)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE ULTIMATE MLP MODEL (TRAINING & FINE-TUNING)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gU3u2SC8OUPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: THE ULTIMATE MLP MODEL (TRAINING & FINE-TUNING)\n",
        "# =============================================================================\n",
        "print(\"--- Starting Ultimate MLP Model Training ---\")\n",
        "\n",
        "# --- 1. Setup (Device, Dataset, Model, Loss) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class SparseDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X, self.y = X, y\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, idx): return torch.tensor(self.X[idx].toarray().flatten(), dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(input_size, 512), nn.ReLU(), nn.Dropout(0.4), nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, 1))\n",
        "    def forward(self, x): return self.layers(x)\n",
        "\n",
        "class SmapeLoss(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__(); self.epsilon = epsilon\n",
        "    def forward(self, y_pred_log, y_true_log):\n",
        "        y_pred, y_true = torch.expm1(y_pred_log), torch.expm1(y_true_log)\n",
        "        num = torch.abs(y_pred - y_true); den = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n",
        "        return torch.mean(num / (den + self.epsilon)) * 100\n",
        "\n",
        "train_loader = DataLoader(SparseDataset(X_sparse, y_numpy), batch_size=256, shuffle=True)\n",
        "model = MLP(X_sparse.shape[1]).to(device)\n",
        "criterion = SmapeLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 2. Initial Training Phase ---\n",
        "print(\"\\n--- Phase 1: Initial Training ---\")\n",
        "initial_epochs = 60\n",
        "for epoch in range(initial_epochs):\n",
        "    model.train(); epoch_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device).view(-1, 1)\n",
        "        outputs = model(features); loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{initial_epochs}], SMAPE Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "\n",
        "# --- 3. Fine-Tuning Phase ---\n",
        "print(\"\\n--- Phase 2: Fine-Tuning ---\")\n",
        "for g in optimizer.param_groups: g['lr'] = 0.0001\n",
        "fine_tune_epochs = 30\n",
        "for epoch in range(fine_tune_epochs):\n",
        "    model.train(); epoch_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device).view(-1, 1)\n",
        "        outputs = model(features); loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Fine-Tuning Epoch [{epoch+1}/{fine_tune_epochs}], SMAPE Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(\"\\n--- Model Training and Fine-Tuning Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWVoBOsANxsN",
        "outputId": "666f07b3-7df5-424d-a5a5-950122332373"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Ultimate MLP Model Training ---\n",
            "\n",
            "--- Phase 1: Initial Training ---\n",
            "Epoch [1/60], SMAPE Loss: 71.8600\n",
            "Epoch [2/60], SMAPE Loss: 61.0244\n",
            "Epoch [3/60], SMAPE Loss: 57.5179\n",
            "Epoch [4/60], SMAPE Loss: 54.5232\n",
            "Epoch [5/60], SMAPE Loss: 52.9310\n",
            "Epoch [6/60], SMAPE Loss: 50.9526\n",
            "Epoch [7/60], SMAPE Loss: 49.3879\n",
            "Epoch [8/60], SMAPE Loss: 47.8896\n",
            "Epoch [9/60], SMAPE Loss: 46.8077\n",
            "Epoch [10/60], SMAPE Loss: 45.9035\n",
            "Epoch [11/60], SMAPE Loss: 44.3808\n",
            "Epoch [12/60], SMAPE Loss: 43.1526\n",
            "Epoch [13/60], SMAPE Loss: 42.0085\n",
            "Epoch [14/60], SMAPE Loss: 41.3769\n",
            "Epoch [15/60], SMAPE Loss: 40.1740\n",
            "Epoch [16/60], SMAPE Loss: 39.7088\n",
            "Epoch [17/60], SMAPE Loss: 38.7813\n",
            "Epoch [18/60], SMAPE Loss: 37.9051\n",
            "Epoch [19/60], SMAPE Loss: 37.3687\n",
            "Epoch [20/60], SMAPE Loss: 37.0439\n",
            "Epoch [21/60], SMAPE Loss: 36.4929\n",
            "Epoch [22/60], SMAPE Loss: 35.9272\n",
            "Epoch [23/60], SMAPE Loss: 35.2846\n",
            "Epoch [24/60], SMAPE Loss: 34.7915\n",
            "Epoch [25/60], SMAPE Loss: 34.1412\n",
            "Epoch [26/60], SMAPE Loss: 34.2182\n",
            "Epoch [27/60], SMAPE Loss: 33.6579\n",
            "Epoch [28/60], SMAPE Loss: 33.3785\n",
            "Epoch [29/60], SMAPE Loss: 32.8786\n",
            "Epoch [30/60], SMAPE Loss: 32.8619\n",
            "Epoch [31/60], SMAPE Loss: 32.6525\n",
            "Epoch [32/60], SMAPE Loss: 31.8824\n",
            "Epoch [33/60], SMAPE Loss: 31.7102\n",
            "Epoch [34/60], SMAPE Loss: 31.5926\n",
            "Epoch [35/60], SMAPE Loss: 31.1050\n",
            "Epoch [36/60], SMAPE Loss: 30.7428\n",
            "Epoch [37/60], SMAPE Loss: 30.7271\n",
            "Epoch [38/60], SMAPE Loss: 30.3681\n",
            "Epoch [39/60], SMAPE Loss: 30.5317\n",
            "Epoch [40/60], SMAPE Loss: 30.0408\n",
            "Epoch [41/60], SMAPE Loss: 29.6959\n",
            "Epoch [42/60], SMAPE Loss: 29.7817\n",
            "Epoch [43/60], SMAPE Loss: 29.5572\n",
            "Epoch [44/60], SMAPE Loss: 29.1413\n",
            "Epoch [45/60], SMAPE Loss: 29.3026\n",
            "Epoch [46/60], SMAPE Loss: 29.1518\n",
            "Epoch [47/60], SMAPE Loss: 28.8835\n",
            "Epoch [48/60], SMAPE Loss: 28.6024\n",
            "Epoch [49/60], SMAPE Loss: 28.4236\n",
            "Epoch [50/60], SMAPE Loss: 28.2941\n",
            "Epoch [51/60], SMAPE Loss: 28.1951\n",
            "Epoch [52/60], SMAPE Loss: 28.1278\n",
            "Epoch [53/60], SMAPE Loss: 27.7179\n",
            "Epoch [54/60], SMAPE Loss: 27.6073\n",
            "Epoch [55/60], SMAPE Loss: 27.6910\n",
            "Epoch [56/60], SMAPE Loss: 27.9313\n",
            "Epoch [57/60], SMAPE Loss: 27.2777\n",
            "Epoch [58/60], SMAPE Loss: 27.2425\n",
            "Epoch [59/60], SMAPE Loss: 27.1945\n",
            "Epoch [60/60], SMAPE Loss: 26.8909\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Fine-Tuning Epoch [1/30], SMAPE Loss: 25.0801\n",
            "Fine-Tuning Epoch [2/30], SMAPE Loss: 24.2016\n",
            "Fine-Tuning Epoch [3/30], SMAPE Loss: 23.8482\n",
            "Fine-Tuning Epoch [4/30], SMAPE Loss: 23.6359\n",
            "Fine-Tuning Epoch [5/30], SMAPE Loss: 23.1912\n",
            "Fine-Tuning Epoch [6/30], SMAPE Loss: 22.9959\n",
            "Fine-Tuning Epoch [7/30], SMAPE Loss: 22.7448\n",
            "Fine-Tuning Epoch [8/30], SMAPE Loss: 22.5066\n",
            "Fine-Tuning Epoch [9/30], SMAPE Loss: 22.3776\n",
            "Fine-Tuning Epoch [10/30], SMAPE Loss: 22.2211\n",
            "Fine-Tuning Epoch [11/30], SMAPE Loss: 22.1074\n",
            "Fine-Tuning Epoch [12/30], SMAPE Loss: 21.7851\n",
            "Fine-Tuning Epoch [13/30], SMAPE Loss: 21.6815\n",
            "Fine-Tuning Epoch [14/30], SMAPE Loss: 21.5753\n",
            "Fine-Tuning Epoch [15/30], SMAPE Loss: 21.3679\n",
            "Fine-Tuning Epoch [16/30], SMAPE Loss: 21.3395\n",
            "Fine-Tuning Epoch [17/30], SMAPE Loss: 21.0669\n",
            "Fine-Tuning Epoch [18/30], SMAPE Loss: 20.9228\n",
            "Fine-Tuning Epoch [19/30], SMAPE Loss: 20.8984\n",
            "Fine-Tuning Epoch [20/30], SMAPE Loss: 20.8521\n",
            "Fine-Tuning Epoch [21/30], SMAPE Loss: 20.8775\n",
            "Fine-Tuning Epoch [22/30], SMAPE Loss: 20.7439\n",
            "Fine-Tuning Epoch [23/30], SMAPE Loss: 20.5890\n",
            "Fine-Tuning Epoch [24/30], SMAPE Loss: 20.6140\n",
            "Fine-Tuning Epoch [25/30], SMAPE Loss: 20.4406\n",
            "Fine-Tuning Epoch [26/30], SMAPE Loss: 20.3012\n",
            "Fine-Tuning Epoch [27/30], SMAPE Loss: 20.3815\n",
            "Fine-Tuning Epoch [28/30], SMAPE Loss: 20.3108\n",
            "Fine-Tuning Epoch [29/30], SMAPE Loss: 20.2667\n",
            "Fine-Tuning Epoch [30/30], SMAPE Loss: 20.2230\n",
            "\n",
            "--- Model Training and Fine-Tuning Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SUBMISSION**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5KJf-9fZOczN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: GENERATE FINAL SUBMISSION\n",
        "# =============================================================================\n",
        "print(\"\\n--- Generating Final Submission File ---\")\n",
        "model.eval(); final_predictions = []\n",
        "batch_size = 512\n",
        "with torch.no_grad():\n",
        "    for i in range(0, X_test_sparse.shape[0], batch_size):\n",
        "        X_batch_sparse = X_test_sparse[i:i + batch_size]\n",
        "        X_batch_dense = torch.tensor(X_batch_sparse.toarray(), dtype=torch.float32).to(device)\n",
        "        outputs = model(X_batch_dense)\n",
        "        final_predictions.append(outputs.cpu())\n",
        "\n",
        "test_predictions_log = torch.cat(final_predictions).numpy()\n",
        "final_predictions = np.expm1(test_predictions_log)\n",
        "\n",
        "if np.isnan(final_predictions).any() or np.isinf(final_predictions).any():\n",
        "    median_pred = np.nanmedian(final_predictions)\n",
        "    final_predictions = np.nan_to_num(final_predictions, nan=median_pred, posinf=median_pred, neginf=median_pred)\n",
        "final_predictions = final_predictions.clip(min=0)\n",
        "\n",
        "submission_df = pd.DataFrame({'sample_id': test_df[SAMPLE_ID_COL], 'price': final_predictions.flatten()})\n",
        "submission_df.to_csv(\"submission_final_experiment.csv\", index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission_final_experiment.csv' created successfully.\")\n",
        "print(\"This represents the culmination of all learned techniques.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIJpd3g8Nxo0",
        "outputId": "c33660cb-7bf9-48de-e0cf-cdda51cf2b56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Final Submission File ---\n",
            "\n",
            "Submission file 'submission_final_experiment.csv' created successfully.\n",
            "This represents the culmination of all learned techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# THE FINAL MOVE: WEIGHTED ENSEMBLE OF CHAMPIONS (CORRECTED)\n",
        "# =============================================================================\n",
        "# This script creates a weighted average of the predictions from your two best\n",
        "# models: the champion Fine-Tuned MLP and the champion CatBoost model.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Creating the Final Weighted Ensemble Submission ---\")\n",
        "\n",
        "# --- 1. Define the paths to your champion submission files ---\n",
        "mlp_champion_file = \"submission_mlp_finetuned.csv\"      # Score: 51.584\n",
        "catboost_champion_file = \"submission_catboost_pipeline_gpu.csv\" # Score: 53.670\n",
        "\n",
        "# --- 2. Define the weights for the blend ---\n",
        "mlp_weight = 0.80\n",
        "catboost_weight = 0.20\n",
        "\n",
        "print(f\"Blending models with weights: MLP={mlp_weight}, CatBoost={catboost_weight}\")\n",
        "\n",
        "try:\n",
        "    # --- 3. Load the submission files ---\n",
        "    df_mlp = pd.read_csv(mlp_champion_file)\n",
        "    df_catboost = pd.read_csv(catboost_champion_file)\n",
        "\n",
        "    # --- 4. CRITICAL: Align the data by sorting ---\n",
        "    df_mlp = df_mlp.sort_values(by='sample_id').reset_index(drop=True)\n",
        "    df_catboost = df_catboost.sort_values(by='sample_id').reset_index(drop=True)\n",
        "\n",
        "    if not df_mlp['sample_id'].equals(df_catboost['sample_id']):\n",
        "        raise ValueError(\"Sample IDs do not match! Cannot create ensemble.\")\n",
        "\n",
        "    # --- 5. Create the Ensemble Prediction ---\n",
        "    ensemble_price = (df_mlp['price'] * mlp_weight) + (df_catboost['price'] * catboost_weight)\n",
        "\n",
        "    # --- 6. Create and save the final submission file ---\n",
        "    df_ensemble = pd.DataFrame({\n",
        "        'sample_id': df_mlp['sample_id'],\n",
        "        'price': ensemble_price\n",
        "    })\n",
        "\n",
        "    # THE FIX: Use 'lower' instead of 'min' for the clip function.\n",
        "    df_ensemble['price'] = df_ensemble['price'].clip(lower=0)\n",
        "\n",
        "    ensemble_filename = \"submission_ensemble_final_80_20.csv\"\n",
        "    df_ensemble.to_csv(ensemble_filename, index=False)\n",
        "\n",
        "    print(f\"\\nEnsemble submission file '{ensemble_filename}' created successfully!\")\n",
        "    print(\"This is your highest-probability final submission. Good luck!\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nERROR: Could not find a submission file. Please check the file paths.\")\n",
        "    print(f\"Missing file: {e.filename}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIAQTISgXjzy",
        "outputId": "83661256-5468-43ba-f039-23e7b18e4c89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating the Final Weighted Ensemble Submission ---\n",
            "Blending models with weights: MLP=0.8, CatBoost=0.2\n",
            "\n",
            "Ensemble submission file 'submission_ensemble_final_80_20.csv' created successfully!\n",
            "This is your highest-probability final submission. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RESNET**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sGBUh13bZIt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: SETUP FOR TABULAR RESNET EXPERIMENT\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "\n",
        "print(\"--- ResNet Experiment: Libraries Imported ---\")\n",
        "\n",
        "# --- Constants & Data Loading (same as before) ---\n",
        "# ... (You can copy the data loading and constants from your previous notebook) ...\n",
        "# Make sure train_df, test_df are loaded.\n",
        "\n",
        "# --- Feature Factory (same as before) ---\n",
        "# ... (Copy your 'create_all_features' function here) ...\n",
        "\n",
        "# --- Apply Feature Factory (same as before) ---\n",
        "# ... (Copy the code to create train_featured and test_featured) ...\n",
        "\n",
        "# --- Data Cleaning and Preprocessing Pipeline (same as before) ---\n",
        "# ... (Copy the code for cleaning and creating the 'transform_pipeline') ...\n",
        "\n",
        "# --- Prepare Data for Modeling (same as before) ---\n",
        "# ... (Copy the code to create X_sparse, y_numpy, and X_test_sparse) ...\n",
        "\n",
        "print(\"Setup and Data Preparation Complete. Data is ready for the ResNet model.\")\n",
        "print(f\"Final feature matrix shape: {X_sparse.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2NaIbdpZIb7",
        "outputId": "370c9c46-6a16-434e-a37c-5252aa557a2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ResNet Experiment: Libraries Imported ---\n",
            "Setup and Data Preparation Complete. Data is ready for the ResNet model.\n",
            "Final feature matrix shape: (74891, 17349)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: THE TABULAR RESNET ARCHITECTURE\n",
        "# =============================================================================\n",
        "print(\"--- Defining the Tabular ResNet Model ---\")\n",
        "\n",
        "# --- 1. Define the Core Residual Block ---\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dropout_rate=0.4):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(input_size)\n",
        "        self.linear1 = nn.Linear(input_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.batch_norm2 = nn.BatchNorm1d(output_size)\n",
        "        self.linear2 = nn.Linear(output_size, output_size)\n",
        "\n",
        "        # This is the key: a linear layer to match dimensions for the skip connection\n",
        "        self.shortcut = nn.Linear(input_size, output_size) if input_size != output_size else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # First part of the block\n",
        "        out = self.batch_norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear1(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Second part of the block\n",
        "        out = self.batch_norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "\n",
        "        # The skip connection\n",
        "        if self.shortcut:\n",
        "            residual = self.shortcut(residual)\n",
        "        out += residual # Add the input (or its projection) to the output\n",
        "\n",
        "        return out\n",
        "\n",
        "# --- 2. Define the Full Tabular ResNet Model ---\n",
        "class TabularResNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TabularResNet, self).__init__()\n",
        "        # An initial \"stem\" to process the raw input\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Stack multiple Residual Blocks to create a deep network\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResBlock(512, 512, dropout_rate=0.4),\n",
        "            ResBlock(512, 256, dropout_rate=0.4),\n",
        "            ResBlock(256, 128, dropout_rate=0.3)\n",
        "        )\n",
        "\n",
        "        # The final \"head\" to produce the output\n",
        "        self.head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "print(\"TabularResNet model architecture defined successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HQv7l6gZIYc",
        "outputId": "a00a6bf5-a53a-4753-f035-bab3ea6d5fe8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Defining the Tabular ResNet Model ---\n",
            "TabularResNet model architecture defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: TRAINING THE TABULAR RESNET\n",
        "# =============================================================================\n",
        "print(\"--- Starting Tabular ResNet Training ---\")\n",
        "\n",
        "# --- 1. Setup (Device, Dataset, Loss) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Re-use the memory-efficient SparseDataset from our previous notebook\n",
        "class SparseDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X, self.y = X, y\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, idx): return torch.tensor(self.X[idx].toarray().flatten(), dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "# Re-use the custom SMAPE Loss\n",
        "class SmapeLoss(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__(); self.epsilon = epsilon\n",
        "    def forward(self, y_pred_log, y_true_log):\n",
        "        y_pred, y_true = torch.expm1(y_pred_log), torch.expm1(y_true_log)\n",
        "        num = torch.abs(y_pred - y_true); den = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n",
        "        return torch.mean(num / (den + self.epsilon)) * 100\n",
        "\n",
        "# --- 2. Initialize and Train ---\n",
        "train_loader = DataLoader(SparseDataset(X_sparse, y_numpy), batch_size=256, shuffle=True)\n",
        "model = TabularResNet(X_sparse.shape[1]).to(device) # <-- The only change is here!\n",
        "criterion = SmapeLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 3. Training Loop (Initial + Fine-Tuning) ---\n",
        "print(\"\\n--- Phase 1: Initial Training ---\")\n",
        "initial_epochs = 50 # We can start with slightly fewer epochs as ResNet can learn faster\n",
        "for epoch in range(initial_epochs):\n",
        "    model.train(); epoch_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device).view(-1, 1)\n",
        "        outputs = model(features); loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{initial_epochs}], SMAPE Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(\"\\n--- Phase 2: Fine-Tuning ---\")\n",
        "for g in optimizer.param_groups: g['lr'] = 0.0001\n",
        "fine_tune_epochs = 25\n",
        "for epoch in range(fine_tune_epochs):\n",
        "    model.train(); epoch_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device).view(-1, 1)\n",
        "        outputs = model(features); loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Fine-Tuning Epoch [{epoch+1}/{fine_tune_epochs}], SMAPE Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(\"\\n--- ResNet Model Training and Fine-Tuning Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7flMAdLAZIVi",
        "outputId": "297d1a25-dbe6-4f02-bc4b-93aac4341f07"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Tabular ResNet Training ---\n",
            "\n",
            "--- Phase 1: Initial Training ---\n",
            "Epoch [1/50], SMAPE Loss: 63.2917\n",
            "Epoch [2/50], SMAPE Loss: 50.7816\n",
            "Epoch [3/50], SMAPE Loss: 45.5575\n",
            "Epoch [4/50], SMAPE Loss: 42.0091\n",
            "Epoch [5/50], SMAPE Loss: 39.6669\n",
            "Epoch [6/50], SMAPE Loss: 37.5176\n",
            "Epoch [7/50], SMAPE Loss: 35.6128\n",
            "Epoch [8/50], SMAPE Loss: 33.9023\n",
            "Epoch [9/50], SMAPE Loss: 32.5285\n",
            "Epoch [10/50], SMAPE Loss: 31.7153\n",
            "Epoch [11/50], SMAPE Loss: 29.9250\n",
            "Epoch [12/50], SMAPE Loss: 28.9597\n",
            "Epoch [13/50], SMAPE Loss: 28.7841\n",
            "Epoch [14/50], SMAPE Loss: 27.4491\n",
            "Epoch [15/50], SMAPE Loss: 26.3818\n",
            "Epoch [16/50], SMAPE Loss: 25.9854\n",
            "Epoch [17/50], SMAPE Loss: 25.0657\n",
            "Epoch [18/50], SMAPE Loss: 24.5474\n",
            "Epoch [19/50], SMAPE Loss: 24.4491\n",
            "Epoch [20/50], SMAPE Loss: 23.4317\n",
            "Epoch [21/50], SMAPE Loss: 22.9125\n",
            "Epoch [22/50], SMAPE Loss: 22.9072\n",
            "Epoch [23/50], SMAPE Loss: 22.0674\n",
            "Epoch [24/50], SMAPE Loss: 21.9748\n",
            "Epoch [25/50], SMAPE Loss: 21.2685\n",
            "Epoch [26/50], SMAPE Loss: 21.3625\n",
            "Epoch [27/50], SMAPE Loss: 21.0547\n",
            "Epoch [28/50], SMAPE Loss: 20.1834\n",
            "Epoch [29/50], SMAPE Loss: 19.8387\n",
            "Epoch [30/50], SMAPE Loss: 19.2334\n",
            "Epoch [31/50], SMAPE Loss: 19.2813\n",
            "Epoch [32/50], SMAPE Loss: 19.1092\n",
            "Epoch [33/50], SMAPE Loss: 19.1118\n",
            "Epoch [34/50], SMAPE Loss: 18.4091\n",
            "Epoch [35/50], SMAPE Loss: 18.2382\n",
            "Epoch [36/50], SMAPE Loss: 17.9959\n",
            "Epoch [37/50], SMAPE Loss: 17.7652\n",
            "Epoch [38/50], SMAPE Loss: 17.6228\n",
            "Epoch [39/50], SMAPE Loss: 17.5687\n",
            "Epoch [40/50], SMAPE Loss: 17.2476\n",
            "Epoch [41/50], SMAPE Loss: 17.1162\n",
            "Epoch [42/50], SMAPE Loss: 16.9748\n",
            "Epoch [43/50], SMAPE Loss: 16.6556\n",
            "Epoch [44/50], SMAPE Loss: 16.2395\n",
            "Epoch [45/50], SMAPE Loss: 16.1296\n",
            "Epoch [46/50], SMAPE Loss: 15.8696\n",
            "Epoch [47/50], SMAPE Loss: 15.7511\n",
            "Epoch [48/50], SMAPE Loss: 15.5022\n",
            "Epoch [49/50], SMAPE Loss: 15.3810\n",
            "Epoch [50/50], SMAPE Loss: 15.8213\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Fine-Tuning Epoch [1/25], SMAPE Loss: 13.7479\n",
            "Fine-Tuning Epoch [2/25], SMAPE Loss: 13.0080\n",
            "Fine-Tuning Epoch [3/25], SMAPE Loss: 12.5871\n",
            "Fine-Tuning Epoch [4/25], SMAPE Loss: 12.1975\n",
            "Fine-Tuning Epoch [5/25], SMAPE Loss: 11.9164\n",
            "Fine-Tuning Epoch [6/25], SMAPE Loss: 11.5939\n",
            "Fine-Tuning Epoch [7/25], SMAPE Loss: 11.4636\n",
            "Fine-Tuning Epoch [8/25], SMAPE Loss: 11.2660\n",
            "Fine-Tuning Epoch [9/25], SMAPE Loss: 11.0743\n",
            "Fine-Tuning Epoch [10/25], SMAPE Loss: 10.9608\n",
            "Fine-Tuning Epoch [11/25], SMAPE Loss: 10.7932\n",
            "Fine-Tuning Epoch [12/25], SMAPE Loss: 10.7296\n",
            "Fine-Tuning Epoch [13/25], SMAPE Loss: 10.5953\n",
            "Fine-Tuning Epoch [14/25], SMAPE Loss: 10.4564\n",
            "Fine-Tuning Epoch [15/25], SMAPE Loss: 10.3563\n",
            "Fine-Tuning Epoch [16/25], SMAPE Loss: 10.2544\n",
            "Fine-Tuning Epoch [17/25], SMAPE Loss: 10.2102\n",
            "Fine-Tuning Epoch [18/25], SMAPE Loss: 10.1170\n",
            "Fine-Tuning Epoch [19/25], SMAPE Loss: 10.0085\n",
            "Fine-Tuning Epoch [20/25], SMAPE Loss: 9.9117\n",
            "Fine-Tuning Epoch [21/25], SMAPE Loss: 9.9529\n",
            "Fine-Tuning Epoch [22/25], SMAPE Loss: 9.8913\n",
            "Fine-Tuning Epoch [23/25], SMAPE Loss: 9.8884\n",
            "Fine-Tuning Epoch [24/25], SMAPE Loss: 9.7168\n",
            "Fine-Tuning Epoch [25/25], SMAPE Loss: 9.7309\n",
            "\n",
            "--- ResNet Model Training and Fine-Tuning Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submission**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CTZFHNS4ZvwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL STEP: GENERATE SUBMISSION FROM TRAINED RESNET MODEL (CORRECTED)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"\\n--- Generating Final Submission File for ResNet Model ---\")\n",
        "\n",
        "# --- 1. Set the model to evaluation mode ---\n",
        "model.eval()\n",
        "\n",
        "# --- 2. Create a DataLoader for the test set ---\n",
        "test_dataset = SparseDataset(X_test_sparse, np.zeros(X_test_sparse.shape[0]))\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# --- 3. Generate Predictions in Batches ---\n",
        "final_predictions_log = []\n",
        "with torch.no_grad():\n",
        "    for features, _ in test_loader:\n",
        "        features = features.to(device)\n",
        "        outputs = model(features)\n",
        "        final_predictions_log.append(outputs.cpu())\n",
        "\n",
        "test_predictions_log = torch.cat(final_predictions_log).numpy()\n",
        "print(\"Predictions generated successfully.\")\n",
        "\n",
        "# --- 4. Post-process the Predictions ---\n",
        "final_predictions = np.expm1(test_predictions_log)\n",
        "\n",
        "# --- 5. Bulletproof Safety Checks ---\n",
        "if np.isnan(final_predictions).any() or np.isinf(final_predictions).any():\n",
        "    print(\"WARNING: Invalid values (NaN or infinity) found in predictions.\")\n",
        "    median_pred = np.nanmedian(final_predictions)\n",
        "    final_predictions = np.nan_to_num(final_predictions, nan=median_pred, posinf=median_pred, neginf=median_pred)\n",
        "    print(f\"Replaced invalid values with the median prediction: {median_pred:.4f}\")\n",
        "\n",
        "# THE FIX: Use 'min=0' for the NumPy array clip function.\n",
        "final_predictions = final_predictions.clip(min=0)\n",
        "print(\"Clipped all predictions to be non-negative.\")\n",
        "\n",
        "# --- 6. Create and Save the Submission DataFrame ---\n",
        "submission_df_resnet = pd.DataFrame({\n",
        "    'sample_id': test_df['sample_id'],\n",
        "    'price': final_predictions.flatten()\n",
        "})\n",
        "\n",
        "submission_filename = \"submission_tabular_resnet.csv\"\n",
        "submission_df_resnet.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nSubmission file '{submission_filename}' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e7vh1G7ZITD",
        "outputId": "6d039de7-b169-4f4e-906b-05f4eab2a629"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Final Submission File for ResNet Model ---\n",
            "Predictions generated successfully.\n",
            "WARNING: Invalid values (NaN or infinity) found in predictions.\n",
            "Replaced invalid values with the median prediction: 12.9172\n",
            "Clipped all predictions to be non-negative.\n",
            "\n",
            "Submission file 'submission_tabular_resnet.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TabNet**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "x_EDIo9meXG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "radY3XNveWvL",
        "outputId": "cbdc5d93-f521-47e1-864c-1f1db5c1d98e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.3)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENT 2: THE TABNET CHALLENGER\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch\n",
        "\n",
        "print(\"--- Starting TabNet Experiment ---\")\n",
        "\n",
        "# --- This code assumes 'train_cleaned' and 'test_featured' dataframes are already loaded and prepared ---\n",
        "\n",
        "# --- 1. Data Preparation for TabNet ---\n",
        "print(\"Preparing data specifically for TabNet...\")\n",
        "df_train = train_cleaned.copy()\n",
        "df_test = test_featured.copy()\n",
        "\n",
        "categorical_features = ['brand']\n",
        "numerical_features = [\n",
        "    'weight_grams', 'pack_count', 'name_char_count', 'name_word_count',\n",
        "    'name_all_caps_word_count', 'is_organic', 'is_gluten_free', 'is_case', 'is_kosher',\n",
        "    'brand_avg_price', 'brand_product_count'\n",
        "]\n",
        "\n",
        "# Handle missing values robustly\n",
        "for col in numerical_features:\n",
        "    median_val = df_train[col].median()\n",
        "    df_train[col].fillna(median_val, inplace=True)\n",
        "    df_test[col].fillna(median_val, inplace=True)\n",
        "df_train[categorical_features[0]].fillna(\"unknown\", inplace=True)\n",
        "df_test[categorical_features[0]].fillna(\"unknown\", inplace=True)\n",
        "\n",
        "# Label Encode Categorical Features\n",
        "cat_encoders = {}\n",
        "for col in categorical_features:\n",
        "    encoder = LabelEncoder()\n",
        "    combined_series = pd.concat([df_train[col], df_test[col]], axis=0)\n",
        "    encoder.fit(combined_series)\n",
        "    df_train[col] = encoder.transform(df_train[col])\n",
        "    df_test[col] = encoder.transform(df_test[col])\n",
        "    cat_encoders[col] = encoder\n",
        "\n",
        "# Create Final Data Matrices\n",
        "features = numerical_features + categorical_features\n",
        "X_train = df_train[features].values\n",
        "y_train = np.log1p(df_train['price']).values.reshape(-1, 1)\n",
        "X_test = df_test[features].values\n",
        "\n",
        "# Get Categorical Indices and Dimensions for TabNet\n",
        "cat_idxs = [features.index(col) for col in categorical_features]\n",
        "cat_dims = [len(cat_encoders[col].classes_) for col in categorical_features]\n",
        "\n",
        "print(\"Data for TabNet is ready.\")\n",
        "\n",
        "# --- 2. Training the TabNet Model ---\n",
        "print(\"\\n--- Training the TabNetRegressor ---\")\n",
        "tabnet_params = dict(\n",
        "    cat_dims=cat_dims, cat_idxs=cat_idxs, cat_emb_dim=4,\n",
        "    n_d=16, n_a=16, n_steps=5, gamma=1.5,\n",
        "    n_independent=2, n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    mask_type='sparsemax',\n",
        "    device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "model_tabnet = TabNetRegressor(**tabnet_params)\n",
        "\n",
        "# Create a validation set for early stopping\n",
        "X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model_tabnet.fit(\n",
        "    X_train=X_train_fold, y_train=y_train_fold,\n",
        "    eval_set=[(X_val_fold, y_val_fold)],\n",
        "    eval_name=['validation'], eval_metric=['rmse'],\n",
        "    max_epochs=100, patience=20, # Increased patience\n",
        "    batch_size=1024, drop_last=False\n",
        ")\n",
        "\n",
        "print(\"TabNet training complete.\")\n",
        "\n",
        "# --- 3. Generate and Save Submission ---\n",
        "print(\"\\n--- Generating TabNet Submission File ---\")\n",
        "test_predictions_log = model_tabnet.predict(X_test)\n",
        "final_predictions = np.expm1(test_predictions_log)\n",
        "\n",
        "submission_df_tabnet = pd.DataFrame({\n",
        "    'sample_id': df_test['sample_id'],\n",
        "    'price': final_predictions.flatten()\n",
        "})\n",
        "submission_df_tabnet['price'] = submission_df_tabnet['price'].clip(lower=0)\n",
        "submission_df_tabnet.to_csv(\"submission_tabnet.csv\", index=False)\n",
        "\n",
        "print(\"Submission file 'submission_tabnet.csv' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvsJaudGeWrs",
        "outputId": "713c6b5a-d215-44aa-86b2-fb5d9fc8a261"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting TabNet Experiment ---\n",
            "Preparing data specifically for TabNet...\n",
            "Data for TabNet is ready.\n",
            "\n",
            "--- Training the TabNetRegressor ---\n",
            "epoch 0  | loss: 0.95825 | validation_rmse: 0.90799 |  0:00:09s\n",
            "epoch 1  | loss: 0.57886 | validation_rmse: 1.15395 |  0:00:13s\n",
            "epoch 2  | loss: 0.56321 | validation_rmse: 0.79169 |  0:00:17s\n",
            "epoch 3  | loss: 0.55209 | validation_rmse: 0.76093 |  0:00:21s\n",
            "epoch 4  | loss: 0.54167 | validation_rmse: 0.8718  |  0:00:25s\n",
            "epoch 5  | loss: 0.54042 | validation_rmse: 0.78851 |  0:00:29s\n",
            "epoch 6  | loss: 0.53129 | validation_rmse: 0.76944 |  0:00:33s\n",
            "epoch 7  | loss: 0.53157 | validation_rmse: 0.83935 |  0:00:37s\n",
            "epoch 8  | loss: 0.53146 | validation_rmse: 0.81536 |  0:00:40s\n",
            "epoch 9  | loss: 0.52785 | validation_rmse: 0.90442 |  0:00:45s\n",
            "epoch 10 | loss: 0.52382 | validation_rmse: 0.89486 |  0:00:49s\n",
            "epoch 11 | loss: 0.52339 | validation_rmse: 0.79696 |  0:00:52s\n",
            "epoch 12 | loss: 0.52149 | validation_rmse: 0.82034 |  0:00:57s\n",
            "epoch 13 | loss: 0.51627 | validation_rmse: 0.79666 |  0:01:01s\n",
            "epoch 14 | loss: 0.508   | validation_rmse: 0.81525 |  0:01:04s\n",
            "epoch 15 | loss: 0.50684 | validation_rmse: 0.78197 |  0:01:09s\n",
            "epoch 16 | loss: 0.50099 | validation_rmse: 0.81677 |  0:01:12s\n",
            "epoch 17 | loss: 0.50031 | validation_rmse: 0.7723  |  0:01:16s\n",
            "epoch 18 | loss: 0.50109 | validation_rmse: 0.80871 |  0:01:21s\n",
            "epoch 19 | loss: 0.50126 | validation_rmse: 0.80948 |  0:01:24s\n",
            "epoch 20 | loss: 0.49519 | validation_rmse: 0.80438 |  0:01:28s\n",
            "epoch 21 | loss: 0.49179 | validation_rmse: 0.7826  |  0:01:34s\n",
            "epoch 22 | loss: 0.48883 | validation_rmse: 0.78882 |  0:01:38s\n",
            "epoch 23 | loss: 0.48271 | validation_rmse: 0.76889 |  0:01:41s\n",
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 3 and best_validation_rmse = 0.76093\n",
            "TabNet training complete.\n",
            "\n",
            "--- Generating TabNet Submission File ---\n",
            "Submission file 'submission_tabnet.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# THE ULTIMATE ENSEMBLE: ResNet + MLP + CatBoost\n",
        "# =============================================================================\n",
        "# This script creates a weighted blend of our three champion models.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Creating the Ultimate Three-Model Ensemble Submission ---\")\n",
        "\n",
        "# --- 1. Define the paths to your champion submission files ---\n",
        "resnet_file = \"submission_tabular_resnet.csv\"  # Score: 50.861\n",
        "mlp_file = \"submission_mlp_finetuned.csv\"      # Score: 51.584\n",
        "catboost_file = \"submission_catboost_pipeline_gpu.csv\" # Score: 53.670\n",
        "\n",
        "# --- 2. Define the weights for the blend ---\n",
        "# We give the most weight to our best single model, the ResNet.\n",
        "# The MLP is second best, and CatBoost is third, but still valuable for its diversity.\n",
        "resnet_weight = 0.50\n",
        "mlp_weight = 0.30\n",
        "catboost_weight = 0.20\n",
        "\n",
        "print(f\"Blending models with weights: ResNet={resnet_weight}, MLP={mlp_weight}, CatBoost={catboost_weight}\")\n",
        "\n",
        "try:\n",
        "    # --- 3. Load and align the submission files ---\n",
        "    df_resnet = pd.read_csv(resnet_file).sort_values(by='sample_id').reset_index(drop=True)\n",
        "    df_mlp = pd.read_csv(mlp_file).sort_values(by='sample_id').reset_index(drop=True)\n",
        "    df_catboost = pd.read_csv(catboost_file).sort_values(by='sample_id').reset_index(drop=True)\n",
        "\n",
        "    # --- 4. Create the Ensemble Prediction ---\n",
        "    ensemble_price = (df_resnet['price'] * resnet_weight) + \\\n",
        "                     (df_mlp['price'] * mlp_weight) + \\\n",
        "                     (df_catboost['price'] * catboost_weight)\n",
        "\n",
        "    # --- 5. Create and save the final submission file ---\n",
        "    df_ensemble = pd.DataFrame({\n",
        "        'sample_id': df_resnet['sample_id'],\n",
        "        'price': ensemble_price\n",
        "    })\n",
        "    df_ensemble['price'] = df_ensemble['price'].clip(lower=0)\n",
        "\n",
        "    ensemble_filename = \"submission_ultimate_ensemble.csv\"\n",
        "    df_ensemble.to_csv(ensemble_filename, index=False)\n",
        "\n",
        "    print(f\"\\nUltimate Ensemble submission file '{ensemble_filename}' created successfully!\")\n",
        "    print(\"This is your best and final shot. Good luck!\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nERROR: Could not find a submission file. Please check the file paths.\")\n",
        "    print(f\"Missing file: {e.filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8vXiQ9eWpI",
        "outputId": "31cd0e12-96d0-48f6-d5a6-fb3210e59720"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating the Ultimate Three-Model Ensemble Submission ---\n",
            "Blending models with weights: ResNet=0.5, MLP=0.3, CatBoost=0.2\n",
            "\n",
            "Ultimate Ensemble submission file 'submission_ultimate_ensemble.csv' created successfully!\n",
            "This is your best and final shot. Good luck!\n"
          ]
        }
      ]
    }
  ]
}